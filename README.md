## **Building GPT**
This repo is referenced from the [build-nanogpt](https://github.com/karpathy/build-nanogpt) by Andrej Karpathy. Thanks!

### **Papers 📄**  
I am reading these papers:  
✅ [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
☑️ [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)

### **Goals 🎯**
✅ Read the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for baseline of the model architecture.  
✅ Inspect the source code of the GPT-2 model from OpenAI & HuggingFace.  
☑️ Prepare a notebook to experiment the model and outputs as done by Andrej Karpathy.  
☑️ Code the training script for GPT-2 model.  
☑️ Work on optimization and training the model on a example dataset.

### **Github Repositories**
🌐 [build-nanogpt](https://github.com/karpathy/build-nanogpt) - Implementation by Andrej Karpathy.    
🌐 [gpt-2](https://github.com/openai/gpt-2) - TensorFlow implementation of GPT-2 by OpenAI.  
🌐 [modeling-gpt2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py) - PyTorch implementation of GPT-2 by HuggingFace. 
