## **Building GPT**
This repo is referenced from the [build-nanogpt](https://github.com/karpathy/build-nanogpt) by Andrej Karpathy. Thanks!

<div style="display: flex; flex-direction: column; align-items: center;">
<img src="assets/gpt.png" alt="" width="400" height="600">
<p style="text-align: center;">Fig a. GPT architecture</p>
</div>


### **Papers 📄**  
I am reading these papers:  
✅ [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
☑️ [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)  
✅ [Attention is All You Need](https://arxiv.org/abs/1706.03762)  
✅ [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)


### **Goals 🎯**
✅ Read the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for baseline of the model architecture.  
✅ Inspect the source code of the GPT-2 model from OpenAI & HuggingFace.  
✅ Prepare a notebook to experiment the model and outputs as done by Andrej Karpathy.  
✅ Implement the GPT-2 model from scratch with diagrams and explanations.  
✅ Implement the transformer block of the model with attention & FFN.  
✅ Implement the FeedForwardBlock of the model with GELU activation.  
✅ Implement the MultiHeadAttentionBlock of the model from scratch.  
☑️ Code the training script for GPT-2 model.  
☑️ Work on optimization and training the model on a example dataset.  


### **Github Repositories**
🌐 [build-nanogpt](https://github.com/karpathy/build-nanogpt) - Implementation by Andrej Karpathy.    
🌐 [gpt-2](https://github.com/openai/gpt-2) - TensorFlow implementation of GPT-2 by OpenAI.  
🌐 [modeling-gpt2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py) - PyTorch implementation of GPT-2 by HuggingFace.  
🌐 [Meta-llama](https://github.com/ThinamXx/Meta-llama/tree/main) - Implementation of Llama by Thinam Tamang.  

