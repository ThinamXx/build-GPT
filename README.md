## **Building GPT**
This repo is referenced from the [build-nanogpt](https://github.com/karpathy/build-nanogpt) by Andrej Karpathy. Thanks!

### **Papers ğŸ“„**  
I am reading these papers:  
âœ… [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
â˜‘ï¸ [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)

### **Goals ğŸ¯**
âœ… Read the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for baseline of the model architecture.  
âœ… Inspect the source code of the GPT-2 model from OpenAI & HuggingFace.  
â˜‘ï¸ Prepare a notebook to experiment the model and outputs as done by Andrej Karpathy.  
â˜‘ï¸ Code the training script for GPT-2 model.  
â˜‘ï¸ Work on optimization and training the model on a example dataset.

### **Github Repositories**
ğŸŒ [build-nanogpt](https://github.com/karpathy/build-nanogpt) - Implementation by Andrej Karpathy.    
ğŸŒ [gpt-2](https://github.com/openai/gpt-2) - TensorFlow implementation of GPT-2 by OpenAI.  
ğŸŒ [modeling-gpt2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py) - PyTorch implementation of GPT-2 by HuggingFace. 
